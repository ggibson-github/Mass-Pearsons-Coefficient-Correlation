{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Start: 2019-08-09 19:01:26.390327\n",
      "KPIsToNormalizedMatrix Start: 2019-08-09 19:01:26.390327\n",
      "KPIsToNormalizedMatrix End: 2019-08-09 19:01:31.236303\n",
      "\n",
      "MXNet PCC results:\n",
      "Execute Time in Seconds:\n",
      "0.28389835357666016\n",
      "Start Write: 2019-08-09 19:01:31.817892\n",
      "End Write: 2019-08-09 20:26:32.637958\n",
      "Pipeline End: 2019-08-09 20:26:32.862363\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "from time import time\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "device = mx.gpu()\n",
    "\n",
    "\n",
    "\n",
    "def GPUComputeKPIs_PCC(normalizedDataPath, outputPath, doVerify, verifySourcePath, VerifyPlaces):\n",
    "    # This method creates a MXNet Graph of actions that action as a function\n",
    "    # that will take in a table of X days of stock close prices for several stocks.\n",
    "    # This graph function will go through a series of matrix multiplication batches.\n",
    "    # For this to work the data must be pre-L2Normalized.\n",
    "    # This process custom batches the data specifically to avoid computing the whole matrix since\n",
    "    # the upper right and lower left corners of the matrix are duplicates.\n",
    "    # This method is described here: \n",
    "    # https://scholarworks.wmich.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1008&context=pcds_reports\n",
    "\n",
    "    \n",
    "    t1=time()\n",
    "    stocksCSV = pd.read_csv(normalizedDataPath, delimiter='\\t', dtype=np.float64)\n",
    "    # Load all stocks into gpu memory\n",
    "    stocks = mx.nd.array(stocksCSV[:],mx.gpu(), dtype='float64')\n",
    "    # Setup variables\n",
    "    gpu_stocks = mx.sym.Variable('gpu_stocks', dtype='float64')\n",
    "    currentStock = mx.sym.Variable('currentStock', dtype='float64')\n",
    "    stocksRemain = mx.sym.Variable('stocksRemain', dtype='float64')\n",
    "    corr_matrix_window = mx.sym.Variable('corr_matrix_window', dtype='float64')\n",
    "    stock_count = mx.sym.Variable('stock_count')\n",
    "    # Not going to use this third transformation matrix so just give it a generic name as a placeholder\n",
    "    C = mx.sym.Variable('C', dtype='float64')\n",
    "    stock_count = int(stocks.shape[1])\n",
    "    C = mx.sym.zeros((0,stock_count), dtype='float64')\n",
    "    sym_group = []\n",
    "    for i in range(stock_count-1):\n",
    "        # Get the current column containing a stock\n",
    "        currentStock = gpu_stocks.slice_axis(axis=1,begin=i,end=i+1)\n",
    "        \n",
    "        # Get the remaining stocks\n",
    "        stocksRemain = gpu_stocks.slice_axis(axis=1,begin=i+1,end=stock_count)\n",
    "        C = mx.sym.zeros((1,stock_count-(i+1)), dtype='float64')\n",
    "        \n",
    "        # Do the matrix multiplication\n",
    "        corr_matrix_window = mx.symbol.linalg_gemm(currentStock, stocksRemain, C, transpose_a=True)\n",
    "        \n",
    "        # Add these actions to the graph\n",
    "        sym_group.append(corr_matrix_window)\n",
    "\n",
    "    symResults = mx.symbol.Group(sym_group)    \n",
    "\n",
    "    # Now the graph is fully defined and we feed in the stock data and instruct MXNet to use the GPU\n",
    "    workflow = (symResults).bind(mx.gpu(), {'gpu_stocks':stocks})\n",
    "\n",
    "    # Execute the graph workflow to get the final PCC score\n",
    "    results = workflow.forward()\n",
    "\n",
    "    t2=time()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"MXNet PCC results:\")\n",
    "    print(\"Execute Time in Seconds:\")\n",
    "    print(t2-t1)\n",
    "    valuesIndex = 0\n",
    "    ppcValues = []\n",
    "    \n",
    "    # Get data into a format to output\n",
    "    for i in range(len(stocksCSV.columns)-1):\n",
    "            ppcValues.append(results[i][0].asnumpy().astype('float64'))\n",
    "    \n",
    "    print('Start Write: ' + str(datetime.now()))\n",
    "    if os.path.exists(outputPath):\n",
    "        os.remove(outputPath)\n",
    "    with open(outputPath, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)    \n",
    "        \n",
    "        # Are we going to do another PCC the very slow way and add it on as a column so we can verify our values?\n",
    "        # Write out a column header\n",
    "        if doVerify:\n",
    "            writer.writerow(['StockA','StockB','Nvidia_PCC_Method','Python_PCC_Method'])\n",
    "        else:\n",
    "            writer.writerow(['StockA','StockB','PCC'])\n",
    "        \n",
    "        #write out the data rows and optionally verify the values with Pandas corr\n",
    "        for i in range(len(stocksCSV.columns)-1):\n",
    "            symbol = stocksCSV.columns[i]\n",
    "            row = ppcValues[i]\n",
    "            #print(row)\n",
    "            colIndex=0\n",
    "            pcc=-2\n",
    "            \n",
    "            #### This +1 is to skip the \"zero\" value used to initialize the array in the GPU\n",
    "            for k in range(i+1, len(stocksCSV.columns)):\n",
    "                symbol2 = stocksCSV.columns[k]\n",
    "\n",
    "                if doVerify == True:\n",
    "                    pcc=VerifyPCC(verifySourcePath, symbol,symbol2)\n",
    "                if doVerify == False or (doVerify == True and pcc != None):\n",
    "                    gpuValue = row[colIndex]\n",
    "                    colIndex += 1\n",
    "\n",
    "                \n",
    "                if pcc == None:\n",
    "                    pcc = -2\n",
    "                elif isinstance(pcc, int) == False:\n",
    "                    pcc = pcc.item()\n",
    "                \n",
    "                if doVerify == True and truncate(pcc,VerifyPlaces) != truncate(gpuValue,VerifyPlaces):\n",
    "                    print(\"No match: \")\n",
    "                    print(str(truncate(pcc,VerifyPlaces)) + \" != \" + str(truncate(gpuValue,VerifyPlaces)))\n",
    "                    print(symbol)\n",
    "                    print(symbol2)\n",
    "                    sys.exit(0)\n",
    "                if doVerify == True:\n",
    "                    writer.writerow([symbol,symbol2,str(gpuValue).strip(),pcc])\n",
    "                    valuesIndex += 1\n",
    "                elif doVerify == False:\n",
    "                    writer.writerow([symbol,symbol2,gpuValue])\n",
    "                    valuesIndex += 1\n",
    "                    \n",
    "              \n",
    "    print('End Write: ' + str(datetime.now()))    \n",
    "\n",
    "\n",
    "def VerifyPCC(verifySourcePath, StockA,StockB):\n",
    "    # Do a Pandas correlation on the close values between two stock files\n",
    "    valueColumnName=\"close\"\n",
    "    stock_A = pd.read_csv(verifySourcePath+StockA+\".csv\", delimiter=\",\", quotechar='\"',usecols=[valueColumnName], dtype={valueColumnName:np.float64})\n",
    "    stock_B = pd.read_csv(verifySourcePath+StockB+\".csv\", delimiter=\",\", quotechar='\"',usecols=[valueColumnName], dtype={valueColumnName:np.float64})\n",
    "    RowCount = len(stock_A.index)\n",
    "\n",
    "  \n",
    "    # Keep first RowCount rows after Start\n",
    "    stock_A = stock_A[0:RowCount]\n",
    "   \n",
    "    stock_B = stock_B[0:RowCount]\n",
    "\n",
    "    # Now both arrays contain a cleaned list of 100 items starting on the same date (startingKey)\n",
    "    df = pd.DataFrame({'A': stock_A[valueColumnName].astype('float64'), 'B': stock_B[valueColumnName].astype('float64')})\n",
    "    return df['A'].corr(df['B'])\n",
    "  \n",
    "    \n",
    "def truncate(f, n):\n",
    "    # Truncates/pads a float f to n decimal places without rounding\n",
    "    s = '{}'.format(f)\n",
    "    if 'e' in s or 'E' in s:\n",
    "        return '{0:.{1}f}'.format(f, n)\n",
    "    i, p, d = s.partition('.')\n",
    "    return '.'.join([i, (d+'0'*n)[:n]])\n",
    "\n",
    "    \n",
    "def KPIsToNormalizedMatrix(input_path, output_path_csv):\n",
    "    # This creates a matrix from a folder of files containing stock data (one file per stock) where the close prices\n",
    "    # are L2Normalized to prepare the data for the matrix multiplications on the GPU later\n",
    "    \n",
    "    KeyColumn = \"timestamp\"\n",
    "    ValueColumn = \"close\"\n",
    "    Start = 0\n",
    "    RowCount = 100\n",
    "    OutputTable = None\n",
    "    workflows = []\n",
    "    symbols = []\n",
    "\n",
    "    print('KPIsToNormalizedMatrix Start: ' + str(datetime.now()))\n",
    "\n",
    "    directory = os.fsencode(input_path)\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "\n",
    "        Start = 0\n",
    "\n",
    "        filename = os.path.join(input_path, file.decode(\"utf-8\"))\n",
    "        symbol = file.decode(\"utf-8\").replace(\".csv\",\"\")\n",
    "        symbols.append(symbol)\n",
    "\n",
    "        # Read in the key and value fields for the KPI\n",
    "        dataFrame = pd.read_csv(filename, header=0, usecols=[KeyColumn, ValueColumn], dtype={ValueColumn:np.float64})\n",
    "\n",
    "        # Keep first RowCount rows after Start\n",
    "        dataFrame = dataFrame[Start:Start+RowCount]\n",
    "\n",
    "\n",
    "\n",
    "        # Copy the data to the gpu and calculate the normalized variances for each value\n",
    "        stock = mx.nd.array([dataFrame[ValueColumn].tolist()], device, dtype='float64')\n",
    "\n",
    "        gpu_stock = mx.sym.Variable('gpu_stock', dtype='float64')\n",
    "        mean = mx.sym.Variable('mean', dtype='float64')\n",
    "        mean_array = mx.sym.Variable('mean_array', dtype='float64')\n",
    "        variance = mx.sym.Variable('variance', dtype='float64')\n",
    "        L2Norm = mx.sym.Variable('L2Norm', dtype='float64')\n",
    "        normalizedStock = mx.sym.Variable('normalizedStock', dtype='float64')\n",
    "\n",
    "\n",
    "        # Calc the mean across all the stock closing prices for the 100 days\n",
    "        mean = gpu_stock.mean()\n",
    "        # Make an array (i.e. vector or 1D matrix) that is the same shape as the stock array \n",
    "        # and that contains only duplicates of the mean value in each slot \n",
    "        # (each slot of this array will be subtracted from an array slot in the stock array)\n",
    "        mean_array = mx.sym.reshape(mx.sym.repeat(mean, 100), shape=(1,100))\n",
    "\n",
    "        # This looks like simple subtraction, however it is actually an array minus an array\n",
    "        # This results in another array where each slot contains the result of same slots in the\n",
    "        # other two arrays (gpu_stock - mean_array) being subtracted\n",
    "\n",
    "        # By subtracting the mean what is left is just the amount the closing price varies from day to day\n",
    "        variance = gpu_stock - mean_array\n",
    "\n",
    "        # See notes and reference on what the L2 Normalization equation is about\n",
    "        normalizedStock = mx.symbol.L2Normalization(variance)\n",
    "\n",
    "        # Now the graph is fully defined and we feed in the stock data and instruct MXNet to use the gpu\n",
    "        workflow = (normalizedStock).bind(device, {'gpu_stock':stock})\n",
    "\n",
    "        # Execute the graph workflow to get the final normalized results\n",
    "        results = workflow.forward()\n",
    "        workflows.append(results)\n",
    "\n",
    "        # Loop through all data and kick off the workflows before trying to retrieve any results.\n",
    "        # This way parallel execution can go on as long as possible before blocking to get a result\n",
    "        # In fact we might be lucky enough to have the results already waiting on us by time \n",
    "        # we access them (by looping over the workflows) so that there will be no blocking\n",
    "\n",
    "        continue\n",
    "\n",
    "    # Lets go get our results from the workflows which have been executing in the background\n",
    "    for i in range(len(workflows)):\n",
    "        symbol = symbols[i]\n",
    "        results = workflows[i][0]\n",
    "        # We will append each KPI array as a column to a 100 row matrix (each row is a timeseries snapshot)\n",
    "        if type(OutputTable) == type(None) or OutputTable.empty:\n",
    "            # Create new DataFrame with the first column\n",
    "            OutputTable = pd.DataFrame(results[0].asnumpy().astype('float64').tolist(), columns=[symbol], dtype=np.float64)\n",
    "        else:\n",
    "            # Append the new column \n",
    "            a = results[0].asnumpy().astype('float64').tolist()\n",
    "            OutputTable[symbol] = a\n",
    "            \n",
    "    OutputTable.dropna()\n",
    "    \n",
    "    OutputTable.to_csv(output_path_csv, sep='\\t', encoding='utf-8', index=False)\n",
    "    print('KPIsToNormalizedMatrix End: ' + str(datetime.now()))\n",
    "    \n",
    "            \n",
    "        \n",
    "rootPath = \"E:\\\\Job7\\\\\"\n",
    "sourceDataPath = \"E:\\\\Job7\\\\DataSource\\\\\"\n",
    "cleanedDataPath = \"E:\\\\Job7\\\\CleanedData\\\\\"\n",
    "normalizedFileName = \"NormalizedKPIValues.csv\"\n",
    "resultsFileName = \"Results.csv\"\n",
    "DoVerify = True\n",
    "# 100 days of data for each stock\n",
    "RowCount = 100\n",
    "VerifyPlaces = 5\n",
    "\n",
    "\n",
    "print('Pipeline Start: ' + str(datetime.now()))\n",
    "\n",
    "# Convert folder of files into a normalized table of KPIs\n",
    "KPIsToNormalizedMatrix(cleanedDataPath, rootPath + normalizedFileName)\n",
    "\n",
    "# GPU compute table of normalized KPIs to PCC values\n",
    "GPUComputeKPIs_PCC(rootPath + normalizedFileName, rootPath + resultsFileName, DoVerify, cleanedDataPath, VerifyPlaces, RowCount)\n",
    "\n",
    "print('Pipeline End: ' + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
